<!DOCTYPE html>
<html lang="ja">
  <head>
    <title>MIRU2024に行ってきました | Shin</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="MIRU2024に参加してきました!! 🔗8/6~9の4日間、熊本城ホールで開催されたMIRU2024に参加してきました。
今回はB4の時の研究にM1の4, 5月での研究成果をアップデートした内容でポスター発表を行い、様々な議論や新しい人脈を作ることができました。
正直なところ、発表前は直近の6月にあったCVPRで大幅に性能を更新した研究が発表されたことや、自分の研究に納得いっていなかったことから、かなりモチベーションが下がっていました。
しかし、実際に参加して、いろいろな人の研究を見ることや、実際に議論することを通してモチベーションが高まっていき、また自分の研究に向き合うエネルギーをもらえたと思います。
これは昨年聴講に行かせていただいた、ICCV2023でも同じようにモチベーションの向上を感じられていて、外部での聴講・発表はモチベーションに繋がるものだと思っているので、他の研究室メンバーも外部に行くチャンスをつかみに行ってほしいと思ってます。
また、先生方といろいろ研究についてお話をさせていただいたのですが、Rejectされてもいいからとにかく論文を書いて出してみろと言っていただけました。
自分も含め、研究室内では自分の研究レベルじゃまだまだ論文投稿できないと思っていると思いますが、打席に立たなければ打率は出ないということで、とりあえず出してみるところから始めたいと思います。
ということでVISAPP2025出します。
あとは、マイルストーンもかねて投稿する学会を決めて、それまでに成果出して投稿するようにスケジュールを組まないと、永遠と研究終わらないようね、という話もしていただきましたので、頑張りましょう。
以下、チュートリアルセッションとオーラルセッションの一言まとめです。オーラルセッションについては、勉強不足で理解できなかったところや間違った解釈のある部分もあるとは思いますが、ご容赦ください。
1日目 🔗チュートリアルセッション 🔗 自動運転のためのビジョン技術
自動運転のためのビジョン技術に関するチュートリアル。end2end型アプローチが主流で、UniADやPARA-DRIVEなどの最新研究が紹介される。カメラ、lidar、ミリ波レーダーなどのセンサを使用し、マッピング、物体追跡、運動予測、行動計画などのタスクを統合。鳥瞰図表現やcross attention、deformable attentionなどの技術も解説。エッジケース対策や消費電力の制約、セーフティクリティカルな側面も議論される
質疑応答としては、リアルタイム性についてや、大規模モデルの是非について
ビジョンのための評価方法 評価方法論は、優れたモデルの特定や研究開発の進捗測定、経験的な知識の取得を目的とするが、ベンチマークでの性能が実アプリケーションでの有用性を保証しない。評価が上手くいかない理由として、評価のズレや方法の悪さ、不適切な運用が挙げられる。
様々なセンサやモダリティを用いたシーン状態推定 様々なセンサやモダリティを用いたシーン状態推定では、RGBセンサの課題（個人情報保護、暗所や遮蔽に弱い、電力・メモリの必要性）を解決するために、イベントカメラ、transient image、無線信号、音響信号、ミリ波信号が提案されている。イベントカメラは輝度変化を記録し、省電力・メモリで高時間分解能を持つ。transient imageは光の到来時刻と強度を記録し、深度推定やnon light of sightイメージングに利用される。音響信号とミリ波信号は暗所耐性と遮蔽耐性があり、3次元姿勢推定や霧越しのイメージングに応用されている。
2日目 🔗OS-1A 🔗Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model 🔗2日目で1番よかった発表です
タスク: どの要素をどこに、どんな大きさで配置するかというレイアウト生成問題。
ポイント: 離散拡散モデルには修正能力がないが、MASKから予測することはできるため、修正すべき部分を見つけてMASKに変換してから修正を促す。
巨大バッグに対するLearning from Label Proportionのための理論的ラベル比率摂動 🔗タスク: バッグサイズが大きいときにラベル比率学習(LLP)がGPUのメモリの制約上困難。
ポイント: オリジナルバッグからサンプリングしミニバッグを生成する。この時与えるラベル比率を摂動することによってオリジナルのラベル比率にオーバーフィットすることを抑制。
大規模マルチモーダルモデルをを用いた広告画像の評価・改善 🔗タスク: Click-Through-Rateを用いた広告画像の評価と改善方法の提示の2つの混合タスク。
ポイント: LLaVaを使って5つの評価要素を言語化し、広告画像のCTRを出力するモデルを構築。入力画像をオーギュメントし複数の構成要素が異なる広告画像を生成し、CTRの抽出、ChatGPTで言語化する。
Reliable and Personalized Federated Learning with Prompt-based Method for Visual Question Answering in Medical Domain 🔗わかりませんでした。">
<meta name="generator" content="Hugo 0.131.0">


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">



<link rel="shortcut icon" href="/images/fav.ico" type="image/x-icon" />








  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">←</span>Home</a>
	
	<a href="/posts">Archive</a>
	<a href="/tags">Tags</a>
	<a href="/about">About</a>

	

	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">MIRU2024に行ってきました</h1>

    <div class="tip">
        <time datetime="2024-08-10 00:00:00 &#43;0000 UTC">Aug 10, 2024</time>
        <span class="split">
          ·
        </span>
        <span>
          708 words
        </span>
        <span class="split">
          ·
        </span>
        <span>
          4 minute read
        </span>
    </div>

    
    


    <div class="content">
      <h1 id="miru2024に参加してきました">MIRU2024に参加してきました!! <a href="#miru2024%e3%81%ab%e5%8f%82%e5%8a%a0%e3%81%97%e3%81%a6%e3%81%8d%e3%81%be%e3%81%97%e3%81%9f" class="anchor">🔗</a></h1><p>8/6~9の4日間、熊本城ホールで開催されたMIRU2024に参加してきました。</p>
<p>今回はB4の時の研究にM1の4, 5月での研究成果をアップデートした内容でポスター発表を行い、様々な議論や新しい人脈を作ることができました。</p>
<p>正直なところ、発表前は直近の6月にあったCVPRで大幅に性能を更新した研究が発表されたことや、自分の研究に納得いっていなかったことから、かなりモチベーションが下がっていました。<br>
しかし、実際に参加して、いろいろな人の研究を見ることや、実際に議論することを通してモチベーションが高まっていき、また自分の研究に向き合うエネルギーをもらえたと思います。<br>
これは昨年聴講に行かせていただいた、ICCV2023でも同じようにモチベーションの向上を感じられていて、外部での聴講・発表はモチベーションに繋がるものだと思っているので、他の研究室メンバーも外部に行くチャンスをつかみに行ってほしいと思ってます。<br>
また、先生方といろいろ研究についてお話をさせていただいたのですが、<strong>Rejectされてもいいからとにかく論文を書いて出してみろ</strong>と言っていただけました。<br>
自分も含め、研究室内では自分の研究レベルじゃまだまだ論文投稿できないと思っていると思いますが、打席に立たなければ打率は出ないということで、とりあえず出してみるところから始めたいと思います。<br>
ということでVISAPP2025出します。</p>
<p>あとは、マイルストーンもかねて投稿する学会を決めて、それまでに成果出して投稿するようにスケジュールを組まないと、永遠と研究終わらないようね、という話もしていただきましたので、頑張りましょう。</p>
<p>以下、チュートリアルセッションとオーラルセッションの一言まとめです。オーラルセッションについては、勉強不足で理解できなかったところや間違った解釈のある部分もあるとは思いますが、ご容赦ください。</p>
<h2 id="1日目">1日目 <a href="#1%e6%97%a5%e7%9b%ae" class="anchor">🔗</a></h2><h3 id="チュートリアルセッション">チュートリアルセッション <a href="#%e3%83%81%e3%83%a5%e3%83%bc%e3%83%88%e3%83%aa%e3%82%a2%e3%83%ab%e3%82%bb%e3%83%83%e3%82%b7%e3%83%a7%e3%83%b3" class="anchor">🔗</a></h3><ol>
<li>
<p>自動運転のためのビジョン技術<br>
自動運転のためのビジョン技術に関するチュートリアル。end2end型アプローチが主流で、UniADやPARA-DRIVEなどの最新研究が紹介される。カメラ、lidar、ミリ波レーダーなどのセンサを使用し、マッピング、物体追跡、運動予測、行動計画などのタスクを統合。鳥瞰図表現やcross attention、deformable attentionなどの技術も解説。エッジケース対策や消費電力の制約、セーフティクリティカルな側面も議論される<br>
質疑応答としては、リアルタイム性についてや、大規模モデルの是非について</p>
</li>
<li>
<p>ビジョンのための評価方法
評価方法論は、優れたモデルの特定や研究開発の進捗測定、経験的な知識の取得を目的とするが、ベンチマークでの性能が実アプリケーションでの有用性を保証しない。評価が上手くいかない理由として、評価のズレや方法の悪さ、不適切な運用が挙げられる。</p>
</li>
<li>
<p>様々なセンサやモダリティを用いたシーン状態推定
様々なセンサやモダリティを用いたシーン状態推定では、RGBセンサの課題（個人情報保護、暗所や遮蔽に弱い、電力・メモリの必要性）を解決するために、イベントカメラ、transient image、無線信号、音響信号、ミリ波信号が提案されている。イベントカメラは輝度変化を記録し、省電力・メモリで高時間分解能を持つ。transient imageは光の到来時刻と強度を記録し、深度推定やnon light of sightイメージングに利用される。音響信号とミリ波信号は暗所耐性と遮蔽耐性があり、3次元姿勢推定や霧越しのイメージングに応用されている。</p>
</li>
</ol>
<h2 id="2日目">2日目 <a href="#2%e6%97%a5%e7%9b%ae" class="anchor">🔗</a></h2><h3 id="os-1a">OS-1A <a href="#os-1a" class="anchor">🔗</a></h3><h4 id="layout-corrector-alleviating-layout-sticking-phenomenon-in-discrete-diffusion-model">Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model <a href="#layout-corrector-alleviating-layout-sticking-phenomenon-in-discrete-diffusion-model" class="anchor">🔗</a></h4><p><strong>2日目で1番よかった発表です</strong><br>
タスク: どの要素をどこに、どんな大きさで配置するかというレイアウト生成問題。<br>
ポイント: 離散拡散モデルには修正能力がないが、MASKから予測することはできるため、修正すべき部分を見つけてMASKに変換してから修正を促す。</p>
<h4 id="巨大バッグに対するlearning-from-label-proportionのための理論的ラベル比率摂動">巨大バッグに対するLearning from Label Proportionのための理論的ラベル比率摂動 <a href="#%e5%b7%a8%e5%a4%a7%e3%83%90%e3%83%83%e3%82%b0%e3%81%ab%e5%af%be%e3%81%99%e3%82%8blearning-from-label-proportion%e3%81%ae%e3%81%9f%e3%82%81%e3%81%ae%e7%90%86%e8%ab%96%e7%9a%84%e3%83%a9%e3%83%99%e3%83%ab%e6%af%94%e7%8e%87%e6%91%82%e5%8b%95" class="anchor">🔗</a></h4><p>タスク: バッグサイズが大きいときにラベル比率学習(LLP)がGPUのメモリの制約上困難。<br>
ポイント: オリジナルバッグからサンプリングしミニバッグを生成する。この時与えるラベル比率を摂動することによってオリジナルのラベル比率にオーバーフィットすることを抑制。</p>
<h4 id="大規模マルチモーダルモデルをを用いた広告画像の評価改善">大規模マルチモーダルモデルをを用いた広告画像の評価・改善 <a href="#%e5%a4%a7%e8%a6%8f%e6%a8%a1%e3%83%9e%e3%83%ab%e3%83%81%e3%83%a2%e3%83%bc%e3%83%80%e3%83%ab%e3%83%a2%e3%83%87%e3%83%ab%e3%82%92%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e5%ba%83%e5%91%8a%e7%94%bb%e5%83%8f%e3%81%ae%e8%a9%95%e4%be%a1%e6%94%b9%e5%96%84" class="anchor">🔗</a></h4><p>タスク: Click-Through-Rateを用いた広告画像の評価と改善方法の提示の2つの混合タスク。<br>
ポイント: LLaVaを使って5つの評価要素を言語化し、広告画像のCTRを出力するモデルを構築。入力画像をオーギュメントし複数の構成要素が異なる広告画像を生成し、CTRの抽出、ChatGPTで言語化する。</p>
<h4 id="reliable-and-personalized-federated-learning-with-prompt-based-method-for-visual-question-answering-in-medical-domain">Reliable and Personalized Federated Learning with Prompt-based Method for Visual Question Answering in Medical Domain <a href="#reliable-and-personalized-federated-learning-with-prompt-based-method-for-visual-question-answering-in-medical-domain" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="組合せ最適化問題の画像的扱いによる高精度解法--巡回セールスパーソン問題へのtest-time-augmentationの適用">組合せ最適化問題の画像的扱いによる高精度解法 — 巡回セールスパーソン問題へのTest-Time Augmentationの適用 <a href="#%e7%b5%84%e5%90%88%e3%81%9b%e6%9c%80%e9%81%a9%e5%8c%96%e5%95%8f%e9%a1%8c%e3%81%ae%e7%94%bb%e5%83%8f%e7%9a%84%e6%89%b1%e3%81%84%e3%81%ab%e3%82%88%e3%82%8b%e9%ab%98%e7%b2%be%e5%ba%a6%e8%a7%a3%e6%b3%95--%e5%b7%a1%e5%9b%9e%e3%82%bb%e3%83%bc%e3%83%ab%e3%82%b9%e3%83%91%e3%83%bc%e3%82%bd%e3%83%b3%e5%95%8f%e9%a1%8c%e3%81%b8%e3%81%aetest-time-augmentation%e3%81%ae%e9%81%a9%e7%94%a8" class="anchor">🔗</a></h4><p>タスク: 巡回セールスパーソン問題(TSP)、NP困難な問題なので、近似解の計算をする。<br>
ポイント: TSPの距離行列を画像として扱い、従来で重視していた項目の逆に注目し、入力順序の依存性を利用し、Transformerベースモデルで近似解を計算。</p>
<h4 id="black-box-forgetting">Black-Box Forgetting <a href="#black-box-forgetting" class="anchor">🔗</a></h4><p>タスク: 事前学習済みの大規模モデルにおいて、特定のクラスを選択的に忘却させるタスク。<br>
ポイント: 従来のモデルの内部情報にアクセスできるホワイトボックスう設定ではなく、アクセスできないブラックボックス設定での問題を提唱。</p>
<h4 id="学習ベース両眼ステレオが持つ事前知識のnerfへの導入">学習ベース両眼ステレオが持つ事前知識のNeRFへの導入 <a href="#%e5%ad%a6%e7%bf%92%e3%83%99%e3%83%bc%e3%82%b9%e4%b8%a1%e7%9c%bc%e3%82%b9%e3%83%86%e3%83%ac%e3%82%aa%e3%81%8c%e6%8c%81%e3%81%a4%e4%ba%8b%e5%89%8d%e7%9f%a5%e8%ad%98%e3%81%aenerf%e3%81%b8%e3%81%ae%e5%b0%8e%e5%85%a5" class="anchor">🔗</a></h4><p>タスク: NeRFでは入力画像が少ない場合に精度が低下を改善。<br>
ポイント: NeRFで生成したステレオペアに学習ベース両眼ステレオを適用し視差を推定。その視差を用いて学習視点の画像を変形し、新しい学習画像を生成、3視点の一貫性空各進度を計算し学習に利用。</p>
<h3 id="os-1b">OS-1B <a href="#os-1b" class="anchor">🔗</a></h3><h4 id="視覚言語モデルの特徴空間分析に基づく極少数サンプルによるプロンプトラーニング">視覚言語モデルの特徴空間分析に基づく極少数サンプルによるプロンプトラーニング <a href="#%e8%a6%96%e8%a6%9a%e8%a8%80%e8%aa%9e%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e7%89%b9%e5%be%b4%e7%a9%ba%e9%96%93%e5%88%86%e6%9e%90%e3%81%ab%e5%9f%ba%e3%81%a5%e3%81%8f%e6%a5%b5%e5%b0%91%e6%95%b0%e3%82%b5%e3%83%b3%e3%83%97%e3%83%ab%e3%81%ab%e3%82%88%e3%82%8b%e3%83%97%e3%83%ad%e3%83%b3%e3%83%97%e3%83%88%e3%83%a9%e3%83%bc%e3%83%8b%e3%83%b3%e3%82%b0" class="anchor">🔗</a></h4><p>タスク: 1shotでのプロンプトラーニングの性能低下の改善。<br>
ポイント: 通常のプロンプトラーニングの16shotとの比較し、言語特徴量が近くに位置してしまう問題に対処。言語と画像のペアの特徴量を近づける包摂損失と、異なるカテゴリの言語特徴量を遠くに位置する排他損失を導入。</p>
<h4 id="referring-image-segmentationに向けたclipとsam併用のための特徴量設計">Referring Image Segmentationに向けたCLIPとSAM併用のための特徴量設計 <a href="#referring-image-segmentation%e3%81%ab%e5%90%91%e3%81%91%e3%81%9fclip%e3%81%a8sam%e4%bd%b5%e7%94%a8%e3%81%ae%e3%81%9f%e3%82%81%e3%81%ae%e7%89%b9%e5%be%b4%e9%87%8f%e8%a8%ad%e8%a8%88" class="anchor">🔗</a></h4><p>タスク: 検出対象を指示可能なSAMデコーダへの特徴量設定<br>
ポイント: わかりませんでした。</p>
<h4 id="3d-reconstruction-of-clothed-human-bodies-from-a-few-views-via-normal-maps">3D reconstruction of clothed human bodies from a few views via normal maps <a href="#3d-reconstruction-of-clothed-human-bodies-from-a-few-views-via-normal-maps" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="分布外データの棄却機能を持つモダンホップフィールドネットワーク">分布外データの棄却機能を持つモダンホップフィールドネットワーク <a href="#%e5%88%86%e5%b8%83%e5%a4%96%e3%83%87%e3%83%bc%e3%82%bf%e3%81%ae%e6%a3%84%e5%8d%b4%e6%a9%9f%e8%83%bd%e3%82%92%e6%8c%81%e3%81%a4%e3%83%a2%e3%83%80%e3%83%b3%e3%83%9b%e3%83%83%e3%83%97%e3%83%95%e3%82%a3%e3%83%bc%e3%83%ab%e3%83%89%e3%83%8d%e3%83%83%e3%83%88%e3%83%af%e3%83%bc%e3%82%af" class="anchor">🔗</a></h4><p>タスク: セーフクリティカルなAI追うように置いて、推論根拠の提示が重要。<br>
ポイント: 入力データに対して、記憶させたデータを紐づけるモダンホップフィールドネットワーク(MHN)を状態空間上の確率密度関数の定式化と分布外のデータの状態を原点に移動することによって、分布外のクエリを与えたときに、何かしらの記憶させたデータに結びついてしまうという問題点を改善。</p>
<h4 id="simglue-スケールや回転変化の大きい画像ペアに対するtransformerを用いた特徴点マッチング">SimGlue: スケールや回転変化の大きい画像ペアに対するTransformerを用いた特徴点マッチング <a href="#simglue-%e3%82%b9%e3%82%b1%e3%83%bc%e3%83%ab%e3%82%84%e5%9b%9e%e8%bb%a2%e5%a4%89%e5%8c%96%e3%81%ae%e5%a4%a7%e3%81%8d%e3%81%84%e7%94%bb%e5%83%8f%e3%83%9a%e3%82%a2%e3%81%ab%e5%af%be%e3%81%99%e3%82%8btransformer%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e7%89%b9%e5%be%b4%e7%82%b9%e3%83%9e%e3%83%83%e3%83%81%e3%83%b3%e3%82%b0" class="anchor">🔗</a></h4><p><strong>NECの発表で、実応用していて素晴らしい内容でした。</strong><br>
タスク: Transformerによる特徴点マッチングはスケールや回転の大きい画像は難しい。<br>
ポイント: Positional Encodingに問題があることが分かったので、その前に座標の正規化を導入。</p>
<h4 id="semi-supervised-domain-adaptation-using-class-order-for-severity-classification">Semi-supervised Domain Adaptation Using Class Order for Severity Classification <a href="#semi-supervised-domain-adaptation-using-class-order-for-severity-classification" class="anchor">🔗</a></h4><p>タスク: 重症度分類における半教師付きドメイン適応。<br>
ポイント: ランキング学習を活用した分布のアライメントとクロスドメインランキング学習によって、異なるドメインでの順序付きクラス分類の精度向上。</p>
<h4 id="domain-adaptation-from-sequentially-arriving-target-images-with-online-deep-sets">Domain Adaptation from Sequentially Arriving Target Images with Online Deep Sets <a href="#domain-adaptation-from-sequentially-arriving-target-images-with-online-deep-sets" class="anchor">🔗</a></h4><p>タスク: ターゲットドメインで最小1サンプルのドメイン適応という少サンプルドメイン適応タスク。<br>
ポイント: 疑似少サンプルロスを逐次的に計算することによって、計算コストの問題に対処しつつ、少サンプルドメイン適応の性能向上。</p>
<h3 id="os-1c">OS-1C <a href="#os-1c" class="anchor">🔗</a></h3><h4 id="tag-guidance-free-open-vocabulary-semantic-segmentation">TAG: Guidance-free Open-Vocabulary Semantic Segmentation <a href="#tag-guidance-free-open-vocabulary-semantic-segmentation" class="anchor">🔗</a></h4><p>タスク: 事前に定義されたカテゴリに依存しないセグメンテーションであるOpen Vocabulary Segmentationタスクではアノテーションはいらないが、ガイダンスは必要。<br>
ポイント: 事前学習済みのCLIPとDINOを使い、データベースから特徴を検索することによってガイダンスフリーとなり、結果としてアノテーションとガイダンスフリーなセグメンテーションを達成。</p>
<h4 id="integrating-query-target-relationship-to-zero-shot-composed-image-retrieval-from-masked-image-text-pairs">Integrating Query-target Relationship to Zero-shot Composed Image Retrieval from Masked Image-text Pairs <a href="#integrating-query-target-relationship-to-zero-shot-composed-image-retrieval-from-masked-image-text-pairs" class="anchor">🔗</a></h4><p>タスク: イメージとテキストをクエリとして画像を検索するComposed Image Retrieval。
ポイント: テキストから主語を取得し、テキストでは主語をマスク、画像では主語の部分以外をマスクしたペアで学習。</p>
<h4 id="language-guided-self-supervised-video-summarization-using-text-semantic-matching">Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching <a href="#language-guided-self-supervised-video-summarization-using-text-semantic-matching" class="anchor">🔗</a></h4><p><strong>OS-1Cで気に入った発表です。</strong><br>
タスク: プロンプトによるパーソナライズされたビデオの要約タスク。<br>
ポイント: 動画フレームに対してキャプションを生成し、ChatGPT4でキャプションから要約を実施。この際にプロンプトでパーソナライズ可能であり、その結果から動画フレームを取得し、ビデオを要約。</p>
<h4 id="shapley-valueを用いた3d物体検出の判断根拠の説明4">Shapley Valueを用いた3D物体検出の判断根拠の説明4 <a href="#shapley-value%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f3d%e7%89%a9%e4%bd%93%e6%a4%9c%e5%87%ba%e3%81%ae%e5%88%a4%e6%96%ad%e6%a0%b9%e6%8b%a0%e3%81%ae%e8%aa%ac%e6%98%8e4" class="anchor">🔗</a></h4><p>タスク: 3D物体検出向けのXAI(説明可能なAI)。<br>
ポイント: Shapley Valueを用いて3D物体検出モデルの出力に対する各入力点の寄与度を計算、目的関数を定義し、検出結果の類似度に基づいて重要度を評価。</p>
<h4 id="任意視点における直接大域成分の生成とその質感編集への応用">任意視点における直接・大域成分の生成とその質感編集への応用 <a href="#%e4%bb%bb%e6%84%8f%e8%a6%96%e7%82%b9%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8b%e7%9b%b4%e6%8e%a5%e5%a4%a7%e5%9f%9f%e6%88%90%e5%88%86%e3%81%ae%e7%94%9f%e6%88%90%e3%81%a8%e3%81%9d%e3%81%ae%e8%b3%aa%e6%84%9f%e7%b7%a8%e9%9b%86%e3%81%b8%e3%81%ae%e5%bf%9c%e7%94%a8" class="anchor">🔗</a></h4><p>タスク: 直接・大域成分の生成とのその質感編集。<br>
ポイント: 直接・大域成分から構成される、光源で照らされたシーンから、境界ボケをNeRFで生成した別視点の画像情報を用いて対処し、直接・大域成分を分離、質感編集への応用を実現。</p>
<h4 id="adaptive-block-sparse-regularization-under-arbitrary-linear-transform">Adaptive Block Sparse Regularization under Arbitrary Linear Transform <a href="#adaptive-block-sparse-regularization-under-arbitrary-linear-transform" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="neural-density-distance-fieldを用いたニューラルレンダリングの透過率バウンドおよび高速化">Neural Density-Distance Fieldを用いたニューラルレンダリングの透過率バウンドおよび高速化 <a href="#neural-density-distance-field%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e3%83%8b%e3%83%a5%e3%83%bc%e3%83%a9%e3%83%ab%e3%83%ac%e3%83%b3%e3%83%80%e3%83%aa%e3%83%b3%e3%82%b0%e3%81%ae%e9%80%8f%e9%81%8e%e7%8e%87%e3%83%90%e3%82%a6%e3%83%b3%e3%83%89%e3%81%8a%e3%82%88%e3%81%b3%e9%ab%98%e9%80%9f%e5%8c%96" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h3 id="os-1d">OS-1D <a href="#os-1d" class="anchor">🔗</a></h3><h4 id="画像生成モデルの追加学習に対する敵対的高周波領域攻撃">画像生成モデルの追加学習に対する敵対的高周波領域攻撃 <a href="#%e7%94%bb%e5%83%8f%e7%94%9f%e6%88%90%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e8%bf%bd%e5%8a%a0%e5%ad%a6%e7%bf%92%e3%81%ab%e5%af%be%e3%81%99%e3%82%8b%e6%95%b5%e5%af%be%e7%9a%84%e9%ab%98%e5%91%a8%e6%b3%a2%e9%a0%98%e5%9f%9f%e6%94%bb%e6%92%83" class="anchor">🔗</a></h4><p>タスク: 生成モデルが悪用されフェイク画像が生成される問題。<br>
ポイント: 高周波領域に強いノイズを付与することによるノイズ除去に頑健な生成手法を提案。</p>
<h4 id="co-embedding-font-shapes-and-impressions-by-contrastive-learning">Co-embedding Font Shapes and Impressions by Contrastive Learning <a href="#co-embedding-font-shapes-and-impressions-by-contrastive-learning" class="anchor">🔗</a></h4><p>タスク: フォントと印象の関係性の解明。<br>
ポイント: フォントと印象の共埋め込みによる対応付けにより、フォントと印象に関連性があることを確認。</p>
<h4 id="重症度比較における不確実性と信頼度に基づく教師なしドメイン適応">重症度比較における不確実性と信頼度に基づく教師なしドメイン適応 <a href="#%e9%87%8d%e7%97%87%e5%ba%a6%e6%af%94%e8%bc%83%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8b%e4%b8%8d%e7%a2%ba%e5%ae%9f%e6%80%a7%e3%81%a8%e4%bf%a1%e9%a0%bc%e5%ba%a6%e3%81%ab%e5%9f%ba%e3%81%a5%e3%81%8f%e6%95%99%e5%b8%ab%e3%81%aa%e3%81%97%e3%83%89%e3%83%a1%e3%82%a4%e3%83%b3%e9%81%a9%e5%bf%9c" class="anchor">🔗</a></h4><p>タスク: 教師なしドメイン対応タスク。<br>
ポイント: 不確実性を推定し、ペアを作成、信頼度の高い結果をデータとして新しく追加することによるドメイン対応を実現。</p>
<h4 id="seeking-flat-minima-with-mean-teacher-on-semi--and-weakly-supervised-domain-generalization-for-object-detection">Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection <a href="#seeking-flat-minima-with-mean-teacher-on-semi--and-weakly-supervised-domain-generalization-for-object-detection" class="anchor">🔗</a></h4><p>タスク: ドメインシフトに頑健な物体検出。<br>
ポイント: 単一のラベルありデータと複数のラベルなし/弱ラベル付きデータでMean Teacher学習を実施することで、アノテーションコストを下げつつ学習。Mean Tacher学習では、ドメインシフトに強い平坦な解に到達することが知られている。</p>
<h4 id="微分可能レンダラーを用いたロゴ画像生成">微分可能レンダラーを用いたロゴ画像生成 <a href="#%e5%be%ae%e5%88%86%e5%8f%af%e8%83%bd%e3%83%ac%e3%83%b3%e3%83%80%e3%83%a9%e3%83%bc%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e3%83%ad%e3%82%b4%e7%94%bb%e5%83%8f%e7%94%9f%e6%88%90" class="anchor">🔗</a></h4><p>タスク: 画像の概形とグラフィックを与えたときのベクター画像生成。<br>
ポイント: ベクターからラスターの勾配計算を実現し、3つの損失を導入することによって、形状・テキストの内容の反映・品質の担保を実現。</p>
<h4 id="遠赤外領域におけるbrdfの計測と解析">遠赤外領域におけるBRDFの計測と解析 <a href="#%e9%81%a0%e8%b5%a4%e5%a4%96%e9%a0%98%e5%9f%9f%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8bbrdf%e3%81%ae%e8%a8%88%e6%b8%ac%e3%81%a8%e8%a7%a3%e6%9e%90" class="anchor">🔗</a></h4><p>タスク: 遠赤外領域におけるBRDFの計測<br>
ポイント: 観測光と熱放射光の差を光源点灯時と消灯時の差を取ることによって実現し、これによってBRDFの計算を可能にした。</p>
<h4 id="neuraleaf-neural-parametric-leaf-models-with-shape-and-deformation-disentanglement">NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement <a href="#neuraleaf-neural-parametric-leaf-models-with-shape-and-deformation-disentanglement" class="anchor">🔗</a></h4><p>タスク: 植物の3次元再構成タスク。<br>
ポイント: 3Dスキャンからベースモデルを構築し、葉っぱの物理特性を利用した3Dの変形パターンを取得、ベースモデルに付与することによってデータを作成。</p>
<h3 id="os-1e">OS-1E <a href="#os-1e" class="anchor">🔗</a></h3><h4 id="towards-semantic-driven-initialization-winning-tickets-in-initial-noise">Towards Semantic-Driven Initialization: Winning Tickets in Initial Noise <a href="#towards-semantic-driven-initialization-winning-tickets-in-initial-noise" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="ブロードキャスト積テンソル形状を揃えた要素積演算">ブロードキャスト積：テンソル形状を揃えた要素積演算 <a href="#%e3%83%96%e3%83%ad%e3%83%bc%e3%83%89%e3%82%ad%e3%83%a3%e3%82%b9%e3%83%88%e7%a9%8d%e3%83%86%e3%83%b3%e3%82%bd%e3%83%ab%e5%bd%a2%e7%8a%b6%e3%82%92%e6%8f%83%e3%81%88%e3%81%9f%e8%a6%81%e7%b4%a0%e7%a9%8d%e6%bc%94%e7%ae%97" class="anchor">🔗</a></h4><p>タスク: 新しい掛け算の提案<br>
ポイント: PyTorchやNumpyで形状を指定せずに掛け算ができてしまっていることに問題を感じ、テンソル形状をそろえて要素咳を行う新しい掛け算について提案。</p>
<h4 id="イベントフォーカルスタックを用いた深度推定">イベントフォーカルスタックを用いた深度推定 <a href="#%e3%82%a4%e3%83%99%e3%83%b3%e3%83%88%e3%83%95%e3%82%a9%e3%83%bc%e3%82%ab%e3%83%ab%e3%82%b9%e3%82%bf%e3%83%83%e3%82%af%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e6%b7%b1%e5%ba%a6%e6%8e%a8%e5%ae%9a" class="anchor">🔗</a></h4><p>タスク: イベントフォーカルスタックを入力として、深度画像を出力<br>
ポイント: 焦点ボケや合焦点に着目したイベントのみを用いた深度推定手法で、イベントフォーカルスタックをボクセル化し、時間方向に区切ることで実現。暗所に強い特性を持つ。</p>
<h4 id="スパイキングニューラルネットワークによる画像生成拡散モデル">スパイキングニューラルネットワークによる画像生成拡散モデル <a href="#%e3%82%b9%e3%83%91%e3%82%a4%e3%82%ad%e3%83%b3%e3%82%b0%e3%83%8b%e3%83%a5%e3%83%bc%e3%83%a9%e3%83%ab%e3%83%8d%e3%83%83%e3%83%88%e3%83%af%e3%83%bc%e3%82%af%e3%81%ab%e3%82%88%e3%82%8b%e7%94%bb%e5%83%8f%e7%94%9f%e6%88%90%e6%8b%a1%e6%95%a3%e3%83%a2%e3%83%87%e3%83%ab" class="anchor">🔗</a></h4><p>タスク: スパイク列を利用したニューロンで計算を行うスパイキングニューラルネットワーク(SNN)を使った画像生成。<br>
ポイント: SNNを使った画像生成は初の試みで、シナプス電流学習を提案した。</p>
<h4 id="グリッドベースnerfにおける周波数正則化">グリッドベースNeRFにおける周波数正則化 <a href="#%e3%82%b0%e3%83%aa%e3%83%83%e3%83%89%e3%83%99%e3%83%bc%e3%82%b9nerf%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8b%e5%91%a8%e6%b3%a2%e6%95%b0%e6%ad%a3%e5%89%87%e5%8c%96" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="cyclic-diffusion-modelの提案とその文字画像変換への応用">Cyclic Diffusion Modelの提案とその文字画像変換への応用 <a href="#cyclic-diffusion-model%e3%81%ae%e6%8f%90%e6%a1%88%e3%81%a8%e3%81%9d%e3%81%ae%e6%96%87%e5%ad%97%e7%94%bb%e5%83%8f%e5%a4%89%e6%8f%9b%e3%81%b8%e3%81%ae%e5%bf%9c%e7%94%a8" class="anchor">🔗</a></h4><p>タスク: ドメイン変換のタスク。<br>
ポイント: CycleGANの考え方をSDEditに導入し、手書き画像と活字画像を学習することによって実現。</p>
<h4 id="3d-shape-modeling-with-adaptive-centroidal-voronoi-tesselation-on-signed-distance-field">3D Shape Modeling with Adaptive Centroidal Voronoi Tesselation on Signed Distance Field <a href="#3d-shape-modeling-with-adaptive-centroidal-voronoi-tesselation-on-signed-distance-field" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="説明文生成を補助タスクとする意味的視聴覚ナビゲーション">説明文生成を補助タスクとする意味的視聴覚ナビゲーション <a href="#%e8%aa%ac%e6%98%8e%e6%96%87%e7%94%9f%e6%88%90%e3%82%92%e8%a3%9c%e5%8a%a9%e3%82%bf%e3%82%b9%e3%82%af%e3%81%a8%e3%81%99%e3%82%8b%e6%84%8f%e5%91%b3%e7%9a%84%e8%a6%96%e8%81%b4%e8%a6%9a%e3%83%8a%e3%83%93%e3%82%b2%e3%83%bc%e3%82%b7%e3%83%a7%e3%83%b3" class="anchor">🔗</a></h4><p>ちゃんと読みたいと思った論文。<br>
タスク: ロボットナビゲーションの説明可能性。<br>
ポイント: Transformerベースネットワークで学習し、さらに強化学習を使って学習。ナビゲーションと説明文生成の同時学習の相乗効果を図った。</p>
<h4 id="human-drawable-and-interpretable-adversarial-attack">Human-drawable and Interpretable Adversarial Attack <a href="#human-drawable-and-interpretable-adversarial-attack" class="anchor">🔗</a></h4><p>タスク: 敵対性攻撃の解釈可能化。<br>
ポイント: 御分類が起こるようなベジェ曲線の集合を学習し、人間が落書きとして再現可能な攻撃として敵対的落書きを提案。解釈性の高い形のため、分類器についての洞察を得た。</p>
<h4 id="text-guided-diverse-scene-interaction-synthesis-by-disentangling-actions-from-scenes">Text-Guided Diverse Scene Interaction Synthesis by Disentangling Actions from Scenes <a href="#text-guided-diverse-scene-interaction-synthesis-by-disentangling-actions-from-scenes" class="anchor">🔗</a></h4><p>タスク: シーンとテキストにあったモーション生成<br>
ポイント: キーポイントを抽出し、シーン上に配置。シーンと3DモーションをPOSAという研究で統合、GPTでオブジェクトを指定しモーションを配置、text motion trajectoryからモーション生成するといった流れ。</p>
<h2 id="3日目">3日目 <a href="#3%e6%97%a5%e7%9b%ae" class="anchor">🔗</a></h2><h3 id="os-2a">OS-2A <a href="#os-2a" class="anchor">🔗</a></h3><h4 id="大規模言語モデルを用いた日本語視覚言語モデルの評価方法とベースラインモデルの提案">大規模言語モデルを用いた日本語視覚言語モデルの評価方法とベースラインモデルの提案 <a href="#%e5%a4%a7%e8%a6%8f%e6%a8%a1%e8%a8%80%e8%aa%9e%e3%83%a2%e3%83%87%e3%83%ab%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e6%97%a5%e6%9c%ac%e8%aa%9e%e8%a6%96%e8%a6%9a%e8%a8%80%e8%aa%9e%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e8%a9%95%e4%be%a1%e6%96%b9%e6%b3%95%e3%81%a8%e3%83%99%e3%83%bc%e3%82%b9%e3%83%a9%e3%82%a4%e3%83%b3%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e6%8f%90%e6%a1%88" class="anchor">🔗</a></h4><p>タスク: 英語圏以外においてのVLMの構築方法や性能を評価するためのベンチマークが未整備。<br>
ポイント: 日本語のVLMの性能を評価するHeron-Benchを提案。ベースラインモデルであるHeron-GITがオープンモデルにおいて競争力のある性能を発揮。</p>
<h4 id="object-aware-query-perturbation-for-cross-modal-image-text-retrieval">Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval <a href="#object-aware-query-perturbation-for-cross-modal-image-text-retrieval" class="anchor">🔗</a></h4><p>後で読む。<br>
タスク: 災害発生時の画像検索で、どこで、何が起きているかを把握して地図上に整理するタスク。<br>
ポイント: VLMを後から個々の物体に注目できるように、クエリを摂動させることによって特徴抽出時の注目する場所を指定。</p>
<h4 id="誤りのある順序付きラベルでの自己緩和型共同学習による重症度推定">誤りのある順序付きラベルでの自己緩和型共同学習による重症度推定 <a href="#%e8%aa%a4%e3%82%8a%e3%81%ae%e3%81%82%e3%82%8b%e9%a0%86%e5%ba%8f%e4%bb%98%e3%81%8d%e3%83%a9%e3%83%99%e3%83%ab%e3%81%a7%e3%81%ae%e8%87%aa%e5%b7%b1%e7%b7%a9%e5%92%8c%e5%9e%8b%e5%85%b1%e5%90%8c%e5%ad%a6%e7%bf%92%e3%81%ab%e3%82%88%e3%82%8b%e9%87%8d%e7%97%87%e5%ba%a6%e6%8e%a8%e5%ae%9a" class="anchor">🔗</a></h4><p>後で読む。<br>
タスク: 誤りのある順序付きラベルにおける学習。<br>
ポイント: 誤りのあるハードラベルから順序付きソフトラベルを導出し2つのラベルを使い学習を行う。</p>
<h4 id="事前学習モデルの特徴表現を維持したsingle-shot-foresight-pruning">事前学習モデルの特徴表現を維持したSingle-shot Foresight Pruning <a href="#%e4%ba%8b%e5%89%8d%e5%ad%a6%e7%bf%92%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e7%89%b9%e5%be%b4%e8%a1%a8%e7%8f%be%e3%82%92%e7%b6%ad%e6%8c%81%e3%81%97%e3%81%9fsingle-shot-foresight-pruning" class="anchor">🔗</a></h4><p>タスク: モデルの大規模化に伴う計算リソースの問題。<br>
ポイント: 事前学習を考慮した枝刈り方法を提案。レイヤの重みを摂動させたときの全てのレイヤの出力特徴を考慮。</p>
<h4 id="学習型足切り表による高速な多様近傍探索">学習型足切り表による高速な多様近傍探索 <a href="#%e5%ad%a6%e7%bf%92%e5%9e%8b%e8%b6%b3%e5%88%87%e3%82%8a%e8%a1%a8%e3%81%ab%e3%82%88%e3%82%8b%e9%ab%98%e9%80%9f%e3%81%aa%e5%a4%9a%e6%a7%98%e8%bf%91%e5%82%8d%e6%8e%a2%e7%b4%a2" class="anchor">🔗</a></h4><p>3日目で一番良かった発表です。<br>
タスク: 高速で多様な近似最近傍探索。
ポイント: 閾値によって各点について近い点のIDを表に記録し、クエリに対して近似最近傍探索を行う。絞り込みでは第一候補を採用し、近い点を表引き市候補から削除することを繰り返し、高速で多様な近似最近傍探索を実現。</p>
<h4 id="generalizable-neural-human-renderer">Generalizable Neural Human Renderer <a href="#generalizable-neural-human-renderer" class="anchor">🔗</a></h4><p>タスク: 単眼ビデオからのヒューマンアバター作成。<br>
ポイント: CNNベースのレンダラーを利用することにより、質の高い、高速で、テスト時の最適化がいらないレンダリングを実現。画像の特徴量を3次元空間にサンプリングし、それをターゲットの姿勢に投影、attentionベースのモデルで情報を集約し、CNNベースのレンダラーでレンダリング。</p>
<h4 id="simple-class-relation-helps-generalized-few-shot-semantic-segmentation">Simple Class Relation Helps Generalized Few-Shot Semantic Segmentation <a href="#simple-class-relation-helps-generalized-few-shot-semantic-segmentation" class="anchor">🔗</a></h4><p>タスク: Few-Shotセマンティックセグメンテーション。<br>
ポイント: 既知クラスモデルに画像を入力して出力と正解マスクで共起数の数え上げを全ての訓練画像で行い、共起数の大きさを用いて既知クラスから新規クラス集合へマッピングする。これに基づき、既知クラス毎に新規クラスの集合を含めて追加の学習を実施。</p>
<h3 id="os-2b">OS-2B <a href="#os-2b" class="anchor">🔗</a></h3><h4 id="path-planning-using-language-guided-probabilistic-roadmaps">Path Planning using Language-Guided Probabilistic Roadmaps <a href="#path-planning-using-language-guided-probabilistic-roadmaps" class="anchor">🔗</a></h4><p>タスク: 経路計画のコストの定義が困難。<br>
ポイント: プロンプトでコストを誘導することにより、どのような経路を取りたいかを指定することができる。</p>
<h4 id="物体特徴を捉えるためのfdslとcgを組合せた事前学習">物体特徴を捉えるためのFDSLとCGを組合せた事前学習 <a href="#%e7%89%a9%e4%bd%93%e7%89%b9%e5%be%b4%e3%82%92%e6%8d%89%e3%81%88%e3%82%8b%e3%81%9f%e3%82%81%e3%81%aefdsl%e3%81%a8cg%e3%82%92%e7%b5%84%e5%90%88%e3%81%9b%e3%81%9f%e4%ba%8b%e5%89%8d%e5%ad%a6%e7%bf%92" class="anchor">🔗</a></h4><p>タスク: 実画像を用いた事前学習の著作権やバイアス問題。<br>
ポイント: 一般物体の複雑な輪郭の組み合わせをCGから取得する手法を提案。</p>
<h4 id="画像生成モデルによるカロリー量を考慮した食事画像編集">画像生成モデルによるカロリー量を考慮した食事画像編集 <a href="#%e7%94%bb%e5%83%8f%e7%94%9f%e6%88%90%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ab%e3%82%88%e3%82%8b%e3%82%ab%e3%83%ad%e3%83%aa%e3%83%bc%e9%87%8f%e3%82%92%e8%80%83%e6%85%ae%e3%81%97%e3%81%9f%e9%a3%9f%e4%ba%8b%e7%94%bb%e5%83%8f%e7%b7%a8%e9%9b%86" class="anchor">🔗</a></h4><p>タスク: カロリー量を考慮した食事画像の編集<br>
ポイント: カロリー量とカテゴリ推定から始め、画像上の食事部分を抜き出して、エッジ抽出、リサイズをし、外観の条件付けとして利用する手法。</p>
<h4 id="llavatour-日本語観光データに特化した大規模マルチモーダルモデルの作成">LLaVATour: 日本語観光データに特化した大規模マルチモーダルモデルの作成 <a href="#llavatour-%e6%97%a5%e6%9c%ac%e8%aa%9e%e8%a6%b3%e5%85%89%e3%83%87%e3%83%bc%e3%82%bf%e3%81%ab%e7%89%b9%e5%8c%96%e3%81%97%e3%81%9f%e5%a4%a7%e8%a6%8f%e6%a8%a1%e3%83%9e%e3%83%ab%e3%83%81%e3%83%a2%e3%83%bc%e3%83%80%e3%83%ab%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e4%bd%9c%e6%88%90" class="anchor">🔗</a></h4><p>タスク: 観光分野におけるドメイン知識の獲得やユーザー意見の理解における課題の解決。<br>
ポイント: 日本語観光データに特化したLLMであるLLaVATourを開発に加えて、画像とユーザーレビューからなるマルチモーダルデータセットの構築。</p>
<h4 id="vision-languageモデルを利用した画像分類におけるバイアスの言語的抽出と緩和">Vision-Languageモデルを利用した画像分類におけるバイアスの言語的抽出と緩和 <a href="#vision-language%e3%83%a2%e3%83%87%e3%83%ab%e3%82%92%e5%88%a9%e7%94%a8%e3%81%97%e3%81%9f%e7%94%bb%e5%83%8f%e5%88%86%e9%a1%9e%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8b%e3%83%90%e3%82%a4%e3%82%a2%e3%82%b9%e3%81%ae%e8%a8%80%e8%aa%9e%e7%9a%84%e6%8a%bd%e5%87%ba%e3%81%a8%e7%b7%a9%e5%92%8c" class="anchor">🔗</a></h4><p>タスク: 分類クラスとは無関係な属性の偏りであるデータセットバイアスを過学習してしまう問題の対処。<br>
ポイント: 事前知識を利用しないバイアス緩和手法として、バイアス属性を言語で抽出し、疑似ラベルの付与によってデータの偏りを学習しないようにする。</p>
<h4 id="岩石薄片を用いた不均一分光フィルタによるワンショット分光撮像">岩石薄片を用いた不均一分光フィルタによるワンショット分光撮像 <a href="#%e5%b2%a9%e7%9f%b3%e8%96%84%e7%89%87%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e4%b8%8d%e5%9d%87%e4%b8%80%e5%88%86%e5%85%89%e3%83%95%e3%82%a3%e3%83%ab%e3%82%bf%e3%81%ab%e3%82%88%e3%82%8b%e3%83%af%e3%83%b3%e3%82%b7%e3%83%a7%e3%83%83%e3%83%88%e5%88%86%e5%85%89%e6%92%ae%e5%83%8f" class="anchor">🔗</a></h4><p>タスク: 簡易な光学系によるワンショット分光撮像の実現。<br>
ポイント: 岩石薄片を利用し作成した岩石フィルタをイメージセンサの前に配置し、シーンを符号化する。符号化画像と岩石フィルタの分光透過率を利用した最適化手法により分光画像を復号。</p>
<h4 id="形状特徴量のレンダリングを活用したテクスチャレスな三次元点群に対するカメラ位置姿勢推定">形状特徴量のレンダリングを活用したテクスチャレスな三次元点群に対するカメラ位置姿勢推定 <a href="#%e5%bd%a2%e7%8a%b6%e7%89%b9%e5%be%b4%e9%87%8f%e3%81%ae%e3%83%ac%e3%83%b3%e3%83%80%e3%83%aa%e3%83%b3%e3%82%b0%e3%82%92%e6%b4%bb%e7%94%a8%e3%81%97%e3%81%9f%e3%83%86%e3%82%af%e3%82%b9%e3%83%81%e3%83%a3%e3%83%ac%e3%82%b9%e3%81%aa%e4%b8%89%e6%ac%a1%e5%85%83%e7%82%b9%e7%be%a4%e3%81%ab%e5%af%be%e3%81%99%e3%82%8b%e3%82%ab%e3%83%a1%e3%83%a9%e4%bd%8d%e7%bd%ae%e5%a7%bf%e5%8b%a2%e6%8e%a8%e5%ae%9a" class="anchor">🔗</a></h4><p>タスク: クエリ写真から3Dモデルにおいてどの位置で撮影したかを推定するカメラ位置推定タスク。<br>
ポイント: 低コストな色なし点群からのカメラ位置推定手法として、点群に対して形状特徴量でのレンダリングをする手法。</p>
<h3 id="os-2c">OS-2C <a href="#os-2c" class="anchor">🔗</a></h3><h4 id="linear-calibration-approach-to-knowledge-free-group-robust-classification">Linear Calibration Approach to Knowledge-free Group-Robust Classification <a href="#linear-calibration-approach-to-knowledge-free-group-robust-classification" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="マルチモーダルllmを用いた食事画像からのレシピ生成">マルチモーダルLLMを用いた食事画像からのレシピ生成 <a href="#%e3%83%9e%e3%83%ab%e3%83%81%e3%83%a2%e3%83%bc%e3%83%80%e3%83%abllm%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e9%a3%9f%e4%ba%8b%e7%94%bb%e5%83%8f%e3%81%8b%e3%82%89%e3%81%ae%e3%83%ac%e3%82%b7%e3%83%94%e7%94%9f%e6%88%90" class="anchor">🔗</a></h4><p>タスク: 食材画像からのレシピ生成。<br>
ポイント: LLaVAをベースとしたMLLMs(マルチモーダルLLM)を開発し、画像から食材リストと調理手順の生成を実現。</p>
<h4 id="画像圧縮におけるvqgan活用のための双条件付き学習">画像圧縮におけるVQGAN活用のための双条件付き学習 <a href="#%e7%94%bb%e5%83%8f%e5%9c%a7%e7%b8%ae%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8bvqgan%e6%b4%bb%e7%94%a8%e3%81%ae%e3%81%9f%e3%82%81%e3%81%ae%e5%8f%8c%e6%9d%a1%e4%bb%b6%e4%bb%98%e3%81%8d%e5%ad%a6%e7%bf%92" class="anchor">🔗</a></h4><p>タスク: 生成モデルベースの画像圧縮。<br>
ポイント: VQGANの生成力を引き出す・補う処理に割り振る特徴量の配分方法をデータサイズによって学習。</p>
<h4 id="ライトトランスポート獲得のための照明露光パタンと復号処理の同時学習">ライトトランスポート獲得のための照明・露光パタンと復号処理の同時学習 <a href="#%e3%83%a9%e3%82%a4%e3%83%88%e3%83%88%e3%83%a9%e3%83%b3%e3%82%b9%e3%83%9d%e3%83%bc%e3%83%88%e7%8d%b2%e5%be%97%e3%81%ae%e3%81%9f%e3%82%81%e3%81%ae%e7%85%a7%e6%98%8e%e9%9c%b2%e5%85%89%e3%83%91%e3%82%bf%e3%83%b3%e3%81%a8%e5%be%a9%e5%8f%b7%e5%87%a6%e7%90%86%e3%81%ae%e5%90%8c%e6%99%82%e5%ad%a6%e7%bf%92" class="anchor">🔗</a></h4><p>タスク: 観測装置に到達するまでの光の伝播であるライトトランスポートの取得。<br>
ポイント: 照明・露光パタンと復号処理の同時学習を1x1の畳み込み層を活用することによって実現。</p>
<h4 id="reactive-biasのチューニングによるvitへの人の知見の組み込み">Reactive BiasのチューニングによるViTへの人の知見の組み込み <a href="#reactive-bias%e3%81%ae%e3%83%81%e3%83%a5%e3%83%bc%e3%83%8b%e3%83%b3%e3%82%b0%e3%81%ab%e3%82%88%e3%82%8bvit%e3%81%b8%e3%81%ae%e4%ba%ba%e3%81%ae%e7%9f%a5%e8%a6%8b%e3%81%ae%e7%b5%84%e3%81%bf%e8%be%bc%e3%81%bf" class="anchor">🔗</a></h4><p>タスク: ViTの構造に適した人の知見の組み込みの実現。<br>
ポイント: 各層のself-AttentionへReactive Biasを導入することによって、Attention weightの操作をし、Reactive BiasのFine Tuningを通じて人の知見の組み込みを実現。</p>
<h4 id="グラフベース近似最近傍探索における適応的開始点選択の理論的実証的解析">グラフベース近似最近傍探索における適応的開始点選択の理論的・実証的解析 <a href="#%e3%82%b0%e3%83%a9%e3%83%95%e3%83%99%e3%83%bc%e3%82%b9%e8%bf%91%e4%bc%bc%e6%9c%80%e8%bf%91%e5%82%8d%e6%8e%a2%e7%b4%a2%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8b%e9%81%a9%e5%bf%9c%e7%9a%84%e9%96%8b%e5%a7%8b%e7%82%b9%e9%81%b8%e6%8a%9e%e3%81%ae%e7%90%86%e8%ab%96%e7%9a%84%e5%ae%9f%e8%a8%bc%e7%9a%84%e8%a7%a3%e6%9e%90" class="anchor">🔗</a></h4><p>タスク:  データベースからクエリに最も近いベクトルを探索する近似最近傍探索における開始点選択の理論的・実証的解析。<br>
ポイント: わかりませんでした。</p>
<h4 id="instance-wise-distribution-control-of-text-to-image-diffusion-models">Instance-wise Distribution Control of Text-to-image Diffusion Models <a href="#instance-wise-distribution-control-of-text-to-image-diffusion-models" class="anchor">🔗</a></h4><p>タスク: 学習データのバイアスも学習するため、生成される画像にバリエーションが少ない。<br>
ポイント: オーブンセットな物体検出によるガイダンスをするために、検出物体をクロップし、属性の抽出、分布の取得を行う。抽出した分布とターゲットの分布を利用し学習を行う。</p>
<h3 id="os-2d">OS-2D <a href="#os-2d" class="anchor">🔗</a></h3><h4 id="unsupervised-intrinsic-image-decomposition-with-lidar-intensity-enhanced-training">Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training <a href="#unsupervised-intrinsic-image-decomposition-with-lidar-intensity-enhanced-training" class="anchor">🔗</a></h4><p>タスク: 固有画像分類タスク。<br>
ポイント: 学習時には画像とLiDARのデータを用いて学習を行い、推論時には画像のみを利用する。画像から推論したalbedoをLiDARから推論したalbedoに近づけることによって性能改善。</p>
<h4 id="refining-generative-class-incremental-learning-performance-through-model-forgetting-strategies">Refining Generative Class Incremental Learning Performance through Model Forgetting Strategies <a href="#refining-generative-class-incremental-learning-performance-through-model-forgetting-strategies" class="anchor">🔗</a></h4><p>タスク: 学習データに含まれない画像の生成が困難。<br>
ポイント: 過去の学習を整理・忘却するContinual Learningを提案。</p>
<h4 id="zodi拡散モデルに基づく画像変換を用いたゼロショットドメイン適応">ZoDi:拡散モデルに基づく画像変換を用いたゼロショットドメイン適応 <a href="#zodi%e6%8b%a1%e6%95%a3%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ab%e5%9f%ba%e3%81%a5%e3%81%8f%e7%94%bb%e5%83%8f%e5%a4%89%e6%8f%9b%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e3%82%bc%e3%83%ad%e3%82%b7%e3%83%a7%e3%83%83%e3%83%88%e3%83%89%e3%83%a1%e3%82%a4%e3%83%b3%e9%81%a9%e5%bf%9c" class="anchor">🔗</a></h4><p>タスク: ゼロショットドメイン適応。<br>
ポイント: 元画像のみセグメンテーションマップと元画像に対してノイズを加えた画像をcontrol netでデノイズし、セグメンテーションタスクの学習と特徴量の類似度の最大化を行う。</p>
<h4 id="撮像条件情報を用いたガイダンスを導入した拡散モデルによる光超音波画像の画質改善">撮像条件情報を用いたガイダンスを導入した拡散モデルによる光超音波画像の画質改善 <a href="#%e6%92%ae%e5%83%8f%e6%9d%a1%e4%bb%b6%e6%83%85%e5%a0%b1%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e3%82%ac%e3%82%a4%e3%83%80%e3%83%b3%e3%82%b9%e3%82%92%e5%b0%8e%e5%85%a5%e3%81%97%e3%81%9f%e6%8b%a1%e6%95%a3%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ab%e3%82%88%e3%82%8b%e5%85%89%e8%b6%85%e9%9f%b3%e6%b3%a2%e7%94%bb%e5%83%8f%e3%81%ae%e7%94%bb%e8%b3%aa%e6%94%b9%e5%96%84" class="anchor">🔗</a></h4><p>タスク: 光超音波画像の画質改善。<br>
ポイント: 光超音波画像の撮像条件を利用してガイダンスベースの拡散モデルを提案。単一の画像と複数の画像では品質が異なるので、その間のベクトルを利用。</p>
<h4 id="イベントカメラと運動光源を用いた法線推定">イベントカメラと運動光源を用いた法線推定 <a href="#%e3%82%a4%e3%83%99%e3%83%b3%e3%83%88%e3%82%ab%e3%83%a1%e3%83%a9%e3%81%a8%e9%81%8b%e5%8b%95%e5%85%89%e6%ba%90%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e6%b3%95%e7%b7%9a%e6%8e%a8%e5%ae%9a" class="anchor">🔗</a></h4><p>タスク: 運動光源とイベントカメラを用いた法線計測<br>
ポイント: 運動光源を利用して輝度変化を出し、輝度対数の時間購買に着目し、これを通じて法線の最適化によって計算。</p>
<h4 id="多焦点偏光画像群からの不透明微小物体の形状復元">多焦点偏光画像群からの不透明微小物体の形状復元 <a href="#%e5%a4%9a%e7%84%a6%e7%82%b9%e5%81%8f%e5%85%89%e7%94%bb%e5%83%8f%e7%be%a4%e3%81%8b%e3%82%89%e3%81%ae%e4%b8%8d%e9%80%8f%e6%98%8e%e5%be%ae%e5%b0%8f%e7%89%a9%e4%bd%93%e3%81%ae%e5%bd%a2%e7%8a%b6%e5%be%a9%e5%85%83" class="anchor">🔗</a></h4><p>タスク: 顕微鏡から不透明かつ微細な物体の形状復元。<br>
ポイント: 偏光がテクスチャのようにふるまうことに着目し、陰影、テクスチャ、偏光情報の焦点ボケを活用し合焦度計算を行う。</p>
<h4 id="one-shot-test-time-adaptation-with-textual-inversion">One-shot Test-Time Adaptation with Textual Inversion <a href="#one-shot-test-time-adaptation-with-textual-inversion" class="anchor">🔗</a></h4><p>タスク: 1枚のラベルなしテスト画像によるドメインシフト対応。<br>
ポイント: 画像だけではなく、Textual Inversionを利用し、テキストも同様にシフトすることによってドメイン対応。</p>
<h4 id="動物の顔認識に対する大規模データセット構築">動物の顔認識に対する大規模データセット構築 <a href="#%e5%8b%95%e7%89%a9%e3%81%ae%e9%a1%94%e8%aa%8d%e8%ad%98%e3%81%ab%e5%af%be%e3%81%99%e3%82%8b%e5%a4%a7%e8%a6%8f%e6%a8%a1%e3%83%87%e3%83%bc%e3%82%bf%e3%82%bb%e3%83%83%e3%83%88%e6%a7%8b%e7%af%89" class="anchor">🔗</a></h4><p>タスク: 動物の個体識別のためのデータセット構築。<br>
ポイント: 自動のフィルタリングをした後、人手でアノテーションすることによって、従来の動物に傷をつけることによる個体識別ではなく、画像による個体識別を目指すためのデータセット構築。</p>
<h4 id="pixel-wise-material-classification-using-deep-learning-by-utilizing-transient-responses">Pixel-Wise Material Classification Using Deep Learning by Utilizing Transient Responses <a href="#pixel-wise-material-classification-using-deep-learning-by-utilizing-transient-responses" class="anchor">🔗</a></h4><p>タスク: 材質判別タスク。<br>
ポイント: Transient histogramが物体毎に固有であることから、Transient histogramを使って分類器を学習することによる、材料判別を実現。</p>
<h4 id="how-to-defend-image-text-retrieval-against-adversarial-attacks">How to Defend Image-Text Retrieval against Adversarial Attacks <a href="#how-to-defend-image-text-retrieval-against-adversarial-attacks" class="anchor">🔗</a></h4><p>タスク: Image-Text Retrievalにおける敵体的攻撃への頑健性。<br>
ポイント: 画像と言語のマルチモーダル攻撃に対して、敵対的学習をマルチモーダルな摂動を加えたデータで学習し、言語の粒度の細かさについて、データ拡張を利用して集合レベルでロスを最大化することでITRタスクにおける初の敵対的攻撃に頑健なモデルを提案。</p>
<h2 id="4日目">4日目 <a href="#4%e6%97%a5%e7%9b%ae" class="anchor">🔗</a></h2><h3 id="os-3a">OS-3A <a href="#os-3a" class="anchor">🔗</a></h3><h4 id="偏光計測系と分類器の同時最適化による材質認識">偏光計測系と分類器の同時最適化による材質認識 <a href="#%e5%81%8f%e5%85%89%e8%a8%88%e6%b8%ac%e7%b3%bb%e3%81%a8%e5%88%86%e9%a1%9e%e5%99%a8%e3%81%ae%e5%90%8c%e6%99%82%e6%9c%80%e9%81%a9%e5%8c%96%e3%81%ab%e3%82%88%e3%82%8b%e6%9d%90%e8%b3%aa%e8%aa%8d%e8%ad%98" class="anchor">🔗</a></h4><p>タスク: 材料分類タスク。<br>
ポイント: 偏光計測系における偏光素子の回転角度の組み合わせと分類器を同時最適化して材料認識する手法の提案。</p>
<h4 id="action-agnostic-point-level-supervision-for-temporal-action-detection">Action-Agnostic Point-Level Supervision for Temporal Action Detection <a href="#action-agnostic-point-level-supervision-for-temporal-action-detection" class="anchor">🔗</a></h4><p>後で読む。<br>
タスク: アクションセグメンテーションタスク。<br>
ポイント: ラベルのない動画に対してアノテーションフレームを自動的に抽出し、ラベル付けするAction-agnostic point level supervision手法を提案。</p>
<h4 id="符号化開口と拡散モデルを用いた単一ボケ画像からの深度再構成">符号化開口と拡散モデルを用いた単一ボケ画像からの深度再構成 <a href="#%e7%ac%a6%e5%8f%b7%e5%8c%96%e9%96%8b%e5%8f%a3%e3%81%a8%e6%8b%a1%e6%95%a3%e3%83%a2%e3%83%87%e3%83%ab%e3%82%92%e7%94%a8%e3%81%84%e3%81%9f%e5%8d%98%e4%b8%80%e3%83%9c%e3%82%b1%e7%94%bb%e5%83%8f%e3%81%8b%e3%82%89%e3%81%ae%e6%b7%b1%e5%ba%a6%e5%86%8d%e6%a7%8b%e6%88%90" class="anchor">🔗</a></h4><p>タスク: 単一ボケ画像からの深度再構成。<br>
ポイント: ブラックボックス的回帰ではなく、観測を考慮した再構成処理を行うために、符号化開口と拡散モデルを用いた深度再構成手法を提案。</p>
<h4 id="learning-object-states-from-actions-via-large-language-models">Learning Object States from Actions via Large Language Models <a href="#learning-object-states-from-actions-via-large-language-models" class="anchor">🔗</a></h4><p>タスク: ある時刻における物体の状態を認識するタスク。<br>
ポイント: 教師データが少なくアノテーションコストが高いため、LLMの知識を用いてナレーション内の行動情報に基づき状態ラベルの生成を行った後、アンサンブルを利用したteacherモデルによってスパースなラベルを密なラベルにし、studentモデルで学習を行う。</p>
<h4 id="anatomical-3d-style-transfer-enables-efficient-federated-learning-with-extremely-low-communication-costs">Anatomical 3D Style Transfer Enables Efficient Federated Learning with Extremely Low Communication Costs <a href="#anatomical-3d-style-transfer-enables-efficient-federated-learning-with-extremely-low-communication-costs" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="prototypeに基づいたattentionによるlearning-from-label-proportions">Prototypeに基づいたAttentionによるLearning from Label Proportions <a href="#prototype%e3%81%ab%e5%9f%ba%e3%81%a5%e3%81%84%e3%81%9fattention%e3%81%ab%e3%82%88%e3%82%8blearning-from-label-proportions" class="anchor">🔗</a></h4><p>タスク: バッグのラベル比率からインスタンスの学習をするLLPタスク。<br>
ポイント: バッグ間で共有されるプロトタイプと、バッグ内のインスタンス間でself-attentionを計算するモジュールを提案。</p>
<h4 id="追加学習された拡散モデルに対するメンバーシップ推論">追加学習された拡散モデルに対するメンバーシップ推論 <a href="#%e8%bf%bd%e5%8a%a0%e5%ad%a6%e7%bf%92%e3%81%95%e3%82%8c%e3%81%9f%e6%8b%a1%e6%95%a3%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ab%e5%af%be%e3%81%99%e3%82%8b%e3%83%a1%e3%83%b3%e3%83%90%e3%83%bc%e3%82%b7%e3%83%83%e3%83%97%e6%8e%a8%e8%ab%96" class="anchor">🔗</a></h4><p>わかりませんでした。</p>
<h4 id="latent-optimization-for-training-free-sketch-guided-diffusion">Latent Optimization for Training-Free Sketch-Guided Diffusion <a href="#latent-optimization-for-training-free-sketch-guided-diffusion" class="anchor">🔗</a></h4><p>タスク: 画像レイアウトのスケッチを用いた画像生成。<br>
ポイント: 潜在空間の最適化により、参照画像と生成画像のアライメントによって制御可能な画像生成を実現。</p>
<h4 id="scaling-backwards-minimal-synthetic-pre-training">Scaling Backwards: Minimal Synthetic Pre-training? <a href="#scaling-backwards-minimal-synthetic-pre-training" class="anchor">🔗</a></h4><p>タスク: 事前学習のデータ数の削減。
ポイント: 1枚の画像による事前学習によって大量にデータを使った事前学習と同程度の性能を発揮。</p>
<h4 id="画像超解像における学習データ構築の再考">画像超解像における学習データ構築の再考 <a href="#%e7%94%bb%e5%83%8f%e8%b6%85%e8%a7%a3%e5%83%8f%e3%81%ab%e3%81%8a%e3%81%91%e3%82%8b%e5%ad%a6%e7%bf%92%e3%83%87%e3%83%bc%e3%82%bf%e6%a7%8b%e7%af%89%e3%81%ae%e5%86%8d%e8%80%83" class="anchor">🔗</a></h4><p>タスク: 1枚の画像に対する解像度を上げるタスク。<br>
ポイント: 品質と多様性についてのフィルタリングを行うことによってWeb画像からのデータセット構築を実現。</p>

    </div>

    
        <div class="tags">
            
                <a href="https://sh-in.github.io/tags/research">research</a>
            
        </div>
    
    
    

</section>


    </main>
    
    <footer id="footer">
    
        <div id="social">


    <a class="symbol" href="https://github.com/sh-in" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28"  viewBox="0 0 72 72" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    
    <title>Github</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)">
            <g id="Github" transform="translate(264.000000, 939.000000)">
                <path d="M8,72 L64,72 C68.418278,72 72,68.418278 72,64 L72,8 C72,3.581722 68.418278,-8.11624501e-16 64,0 L8,0 C3.581722,8.11624501e-16 -5.41083001e-16,3.581722 0,8 L0,64 C5.41083001e-16,68.418278 3.581722,72 8,72 Z" id="Rounded" fill="#bbbbbb"></path>
                <path d="M35.9985,13 C22.746,13 12,23.7870921 12,37.096644 C12,47.7406712 18.876,56.7718301 28.4145,59.9584121 C29.6145,60.1797862 30.0525,59.4358488 30.0525,58.7973276 C30.0525,58.2250681 30.0315,56.7100863 30.0195,54.6996482 C23.343,56.1558981 21.9345,51.4693938 21.9345,51.4693938 C20.844,48.6864054 19.2705,47.9454799 19.2705,47.9454799 C17.091,46.4500754 19.4355,46.4801943 19.4355,46.4801943 C21.843,46.6503662 23.1105,48.9634994 23.1105,48.9634994 C25.2525,52.6455377 28.728,51.5823398 30.096,50.9649018 C30.3135,49.4077535 30.9345,48.3460615 31.62,47.7436831 C26.2905,47.1352808 20.688,45.0691228 20.688,35.8361671 C20.688,33.2052792 21.6225,31.0547881 23.1585,29.3696344 C22.911,28.7597262 22.0875,26.3110578 23.3925,22.9934585 C23.3925,22.9934585 25.4085,22.3459017 29.9925,25.4632101 C31.908,24.9285993 33.96,24.6620468 36.0015,24.6515052 C38.04,24.6620468 40.0935,24.9285993 42.0105,25.4632101 C46.5915,22.3459017 48.603,22.9934585 48.603,22.9934585 C49.9125,26.3110578 49.089,28.7597262 48.8415,29.3696344 C50.3805,31.0547881 51.309,33.2052792 51.309,35.8361671 C51.309,45.0917119 45.6975,47.1292571 40.3515,47.7256117 C41.2125,48.4695491 41.9805,49.9393525 41.9805,52.1877301 C41.9805,55.4089489 41.9505,58.0067059 41.9505,58.7973276 C41.9505,59.4418726 42.3825,60.1918338 43.6005,59.9554002 C53.13,56.7627944 60,47.7376593 60,37.096644 C60,23.7870921 49.254,13 35.9985,13" fill="#FFFFFF"></path>
            </g>
        </g>
    </g>
</svg>
    </a>

    <a class="symbol" href="https://x.com/shinnn0722" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28" version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="438.536px" height="438.536px" viewBox="0 0 438.536 438.536" style="enable-background:new 0 0 438.536 438.536;"
	 xml:space="preserve">
<g>
	<path d="M414.41,24.123C398.333,8.042,378.963,0,356.315,0H82.228C59.58,0,40.21,8.042,24.126,24.123
		C8.045,40.207,0.003,59.576,0.003,82.225v274.084c0,22.647,8.042,42.018,24.123,58.102c16.084,16.084,35.454,24.126,58.102,24.126
		h274.084c22.648,0,42.018-8.042,58.095-24.126c16.084-16.084,24.126-35.454,24.126-58.102V82.225
		C438.532,59.576,430.49,40.204,414.41,24.123z M335.471,168.735c0.191,1.713,0.288,4.278,0.288,7.71
		c0,15.989-2.334,32.025-6.995,48.104c-4.661,16.087-11.8,31.504-21.416,46.254c-9.606,14.749-21.074,27.791-34.396,39.115
		c-13.325,11.32-29.311,20.365-47.968,27.117c-18.648,6.762-38.637,10.143-59.953,10.143c-33.116,0-63.76-8.952-91.931-26.836
		c4.568,0.568,9.329,0.855,14.275,0.855c27.6,0,52.439-8.565,74.519-25.7c-12.941-0.185-24.506-4.179-34.688-11.991
		c-10.185-7.803-17.273-17.699-21.271-29.691c4.947,0.76,8.658,1.137,11.132,1.137c4.187,0,9.042-0.76,14.56-2.279
		c-13.894-2.669-25.598-9.562-35.115-20.697c-9.519-11.136-14.277-23.84-14.277-38.114v-0.571
		c10.085,4.755,19.602,7.229,28.549,7.422c-17.321-11.613-25.981-28.265-25.981-49.963c0-10.66,2.758-20.747,8.278-30.264
		c15.035,18.464,33.311,33.213,54.816,44.252c21.507,11.038,44.54,17.227,69.092,18.558c-0.95-3.616-1.427-8.186-1.427-13.704
		c0-16.562,5.853-30.692,17.56-42.399c11.703-11.706,25.837-17.561,42.394-17.561c17.515,0,32.079,6.283,43.688,18.846
		c13.134-2.474,25.892-7.33,38.26-14.56c-4.757,14.652-13.613,25.788-26.55,33.402c12.368-1.716,23.88-4.95,34.537-9.708
		C357.458,149.793,347.462,160.166,335.471,168.735z"/>
</g>
</svg>

    </a>


</div>

    

    <div class="copyright">
    
       © Copyright 
       2024 
       <span class="split">
        <svg fill="#bbbbbb" width="15" height="15" version="1.1" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15px" height="15px" viewBox="0 0 15 15">
  <path d="M13.91,6.75c-1.17,2.25-4.3,5.31-6.07,6.94c-0.1903,0.1718-0.4797,0.1718-0.67,0C5.39,12.06,2.26,9,1.09,6.75&#xA;&#x9;C-1.48,1.8,5-1.5,7.5,3.45C10-1.5,16.48,1.8,13.91,6.75z"/>
</svg>
       </span>
       Shin
    
    </div>

    
</footer>



  </body>
</html>
